{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "AAII_Transformer_Tutorial",
      "provenance": [],
      "collapsed_sections": [
        "0FxbG1zZ01Zf",
        "g2V-uQX70sNg",
        "TCQnBU2t0ZBa",
        "0YU-lxmZ1nfZ",
        "pIh5g8yj2G3t"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_aDMApv63qs",
        "colab_type": "text"
      },
      "source": [
        "# AAII Transformer Tutorial\n",
        "### By David Reiman (github.com/davidreiman)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjqs7ztB7HNW",
        "colab_type": "text"
      },
      "source": [
        "#### Note: you should use a GPU runtime while running this notebook. To do so, select `Runtime` -> `Change runtime type` and select `GPU` under `Hardware accelerator`. You should then be assigned a GPU. You can check this via the `!nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpqWUQIH7nk_",
        "colab_type": "code",
        "outputId": "8a97231b-80b9-4483-81b2-1389e87e5e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 22 01:55:08 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHeEoLUf0gEK",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48JWrYI5xQxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import pickle\n",
        "import zipfile\n",
        "import datetime\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as U\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import collections as col\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from collections import Counter, OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjii1GGpxQxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set()\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "plt.rcParams['figure.figsize'] = (16, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOOlvZmH0cCg",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FxbG1zZ01Zf",
        "colab_type": "text"
      },
      "source": [
        "### Download enwik8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CfIPXS_ElvR",
        "colab_type": "text"
      },
      "source": [
        "enwik8 is a character-level dataset comprising the first $10^9$ bytes of the English Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1I-y_j20cZP",
        "colab_type": "code",
        "outputId": "75db44a3-5057-45a5-f700-e407920b4403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --continue http://mattmahoney.net/dc/enwik8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-21 03:14:11--  http://mattmahoney.net/dc/enwik8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36445475 (35M) [application/zip]\n",
            "Saving to: ‘enwik8.zip’\n",
            "\n",
            "enwik8.zip          100%[===================>]  34.76M   336KB/s    in 1m 48s  \n",
            "\n",
            "2019-11-21 03:16:00 (329 KB/s) - ‘enwik8.zip’ saved [36445475/36445475]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2V-uQX70sNg",
        "colab_type": "text"
      },
      "source": [
        "### Process enwik8 (credit: Salesforce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVX984OWE3U2",
        "colab_type": "text"
      },
      "source": [
        "This will turn all unique characters in enwik8 to their UTF-8 encoding (an 8-bit integer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is-YwPAX0mx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists('train.txt'):\n",
        "    print('Tokenized enwik8 already exists - skipping processing')\n",
        "    sys.exit()\n",
        "\n",
        "data = zipfile.ZipFile('enwik8.zip').read('enwik8')\n",
        "\n",
        "print('Length of enwik8: {}'.format(len(data)))\n",
        "\n",
        "num_test_chars = 5000000\n",
        "\n",
        "train_data = data[: -2 * num_test_chars]\n",
        "valid_data = data[-2 * num_test_chars: -num_test_chars]\n",
        "test_data = data[-num_test_chars:]\n",
        "\n",
        "for fn, part in [('train.txt', train_data), ('valid.txt', valid_data), ('test.txt', test_data)]:\n",
        "    print('{} will have {} bytes'.format(fn, len(part)))\n",
        "    print('- Tokenizing...')\n",
        "    part_str = ' '.join([str(c) if c != ord('\\n') else '\\n' for c in part])\n",
        "    print('- Writing...')\n",
        "    f = open(fn, 'w').write(part_str)\n",
        "    f = open(fn + '.raw', 'wb').write(part)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCQnBU2t0ZBa",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YU-lxmZ1nfZ",
        "colab_type": "text"
      },
      "source": [
        "### Dataset iterator (credit: Zhilin Yang [Transformer-XL])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8UwVWh7xQxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    def __init__(self, special=[], min_freq=0, max_size=None, lower_case=True,\n",
        "                 delimiter=None, vocab_file=None):\n",
        "        self.counter = Counter()\n",
        "        self.special = special\n",
        "        self.min_freq = min_freq\n",
        "        self.max_size = max_size\n",
        "        self.lower_case = lower_case\n",
        "        self.delimiter = delimiter\n",
        "        self.vocab_file = vocab_file\n",
        "\n",
        "    def tokenize(self, line, add_eos=False, add_double_eos=False):\n",
        "        line = line.strip()\n",
        "        # convert to lower case\n",
        "        if self.lower_case:\n",
        "            line = line.lower()\n",
        "\n",
        "        # empty delimiter '' will evaluate False\n",
        "        if self.delimiter == '':\n",
        "            symbols = line\n",
        "        else:\n",
        "            symbols = line.split(self.delimiter)\n",
        "\n",
        "        if add_double_eos: # lm1b\n",
        "            return ['<S>'] + symbols + ['<S>']\n",
        "        elif add_eos:\n",
        "            return symbols + ['<eos>']\n",
        "        else:\n",
        "            return symbols\n",
        "\n",
        "    def count_file(self, path, verbose=False, add_eos=False):\n",
        "        if verbose: print('counting file {} ...'.format(path))\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        sents = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                if verbose and idx > 0 and idx % 500000 == 0:\n",
        "                    print('    line {}'.format(idx))\n",
        "                symbols = self.tokenize(line, add_eos=add_eos)\n",
        "                self.counter.update(symbols)\n",
        "                sents.append(symbols)\n",
        "\n",
        "        return sents\n",
        "\n",
        "    def count_sents(self, sents, verbose=False):\n",
        "        \"\"\"\n",
        "            sents : a list of sentences, each a list of tokenized symbols\n",
        "        \"\"\"\n",
        "        if verbose: print('counting {} sents ...'.format(len(sents)))\n",
        "        for idx, symbols in enumerate(sents):\n",
        "            if verbose and idx > 0 and idx % 500000 == 0:\n",
        "                print('    line {}'.format(idx))\n",
        "            self.counter.update(symbols)\n",
        "\n",
        "    def _build_from_file(self, vocab_file):\n",
        "        self.idx2sym = []\n",
        "        self.sym2idx = OrderedDict()\n",
        "\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                symb = line.strip().split()[0]\n",
        "                self.add_symbol(symb)\n",
        "        self.unk_idx = self.sym2idx['<UNK>']\n",
        "\n",
        "    def build_vocab(self):\n",
        "        if self.vocab_file:\n",
        "            print('building vocab from {}'.format(self.vocab_file))\n",
        "            self._build_from_file(self.vocab_file)\n",
        "            print('final vocab size {}'.format(len(self)))\n",
        "        else:\n",
        "            print('building vocab with min_freq={}, max_size={}'.format(\n",
        "                self.min_freq, self.max_size))\n",
        "            self.idx2sym = []\n",
        "            self.sym2idx = OrderedDict()\n",
        "\n",
        "            for sym in self.special:\n",
        "                self.add_special(sym)\n",
        "\n",
        "            for sym, cnt in self.counter.most_common(self.max_size):\n",
        "                if cnt < self.min_freq: break\n",
        "                self.add_symbol(sym)\n",
        "\n",
        "            print('final vocab size {} from {} unique tokens'.format(\n",
        "                len(self), len(self.counter)))\n",
        "\n",
        "    def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n",
        "            add_double_eos=False):\n",
        "        if verbose: print('encoding file {} ...'.format(path))\n",
        "        assert os.path.exists(path)\n",
        "        encoded = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                if verbose and idx > 0 and idx % 500000 == 0:\n",
        "                    print('    line {}'.format(idx))\n",
        "                symbols = self.tokenize(line, add_eos=add_eos,\n",
        "                    add_double_eos=add_double_eos)\n",
        "                encoded.append(self.convert_to_tensor(symbols))\n",
        "\n",
        "        if ordered:\n",
        "            encoded = torch.cat(encoded)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def encode_sents(self, sents, ordered=False, verbose=False):\n",
        "        if verbose: print('encoding {} sents ...'.format(len(sents)))\n",
        "        encoded = []\n",
        "        for idx, symbols in enumerate(sents):\n",
        "            if verbose and idx > 0 and idx % 500000 == 0:\n",
        "                print('    line {}'.format(idx))\n",
        "            encoded.append(self.convert_to_tensor(symbols))\n",
        "\n",
        "        if ordered:\n",
        "            encoded = torch.cat(encoded)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def add_special(self, sym):\n",
        "        if sym not in self.sym2idx:\n",
        "            self.idx2sym.append(sym)\n",
        "            self.sym2idx[sym] = len(self.idx2sym) - 1\n",
        "            setattr(self, '{}_idx'.format(sym.strip('<>')), self.sym2idx[sym])\n",
        "\n",
        "    def add_symbol(self, sym):\n",
        "        if sym not in self.sym2idx:\n",
        "            self.idx2sym.append(sym)\n",
        "            self.sym2idx[sym] = len(self.idx2sym) - 1\n",
        "\n",
        "    def get_sym(self, idx):\n",
        "        assert 0 <= idx < len(self), 'Index {} out of range'.format(idx)\n",
        "        return self.idx2sym[idx]\n",
        "\n",
        "    def get_idx(self, sym):\n",
        "        if sym in self.sym2idx:\n",
        "            return self.sym2idx[sym]\n",
        "        else:\n",
        "            # print('encounter unk {}'.format(sym))\n",
        "            assert '<eos>' not in sym\n",
        "            assert hasattr(self, 'unk_idx')\n",
        "            return self.sym2idx.get(sym, self.unk_idx)\n",
        "\n",
        "    def get_symbols(self, indices):\n",
        "        return [self.get_sym(idx) for idx in indices]\n",
        "\n",
        "    def get_indices(self, symbols):\n",
        "        return [self.get_idx(sym) for sym in symbols]\n",
        "\n",
        "    def convert_to_tensor(self, symbols):\n",
        "        return torch.LongTensor(self.get_indices(symbols))\n",
        "\n",
        "    def convert_to_sent(self, indices, exclude=None):\n",
        "        if exclude is None:\n",
        "            return ' '.join([self.get_sym(idx) for idx in indices])\n",
        "        else:\n",
        "            return ' '.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2sym)\n",
        "\n",
        "\n",
        "class LMOrderedIterator(object):\n",
        "    def __init__(self, data, bsz, bptt, device='cpu', ext_len=None):\n",
        "        \"\"\"\n",
        "            data -- LongTensor -- the LongTensor is strictly ordered\n",
        "        \"\"\"\n",
        "        self.bsz = bsz\n",
        "        self.bptt = bptt\n",
        "        self.ext_len = ext_len if ext_len is not None else 0\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "        self.n_step = data.size(0) // bsz\n",
        "\n",
        "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "        data = data.narrow(0, 0, self.n_step * bsz)\n",
        "\n",
        "        # Evenly divide the data across the bsz batches.\n",
        "        self.data = data.view(bsz, -1).t().contiguous().to(device)\n",
        "\n",
        "        # Number of mini-batches\n",
        "        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n",
        "\n",
        "    def get_batch(self, i, bptt=None):\n",
        "        if bptt is None: bptt = self.bptt\n",
        "        seq_len = min(bptt, self.data.size(0) - 1 - i)\n",
        "\n",
        "        end_idx = i + seq_len\n",
        "        beg_idx = max(0, i - self.ext_len)\n",
        "\n",
        "        data = self.data[beg_idx:end_idx]\n",
        "        target = self.data[i+1:i+1+seq_len]\n",
        "\n",
        "        return data, target, seq_len\n",
        "\n",
        "    def get_fixlen_iter(self, start=0):\n",
        "        for i in range(start, self.data.size(0) - 1, self.bptt):\n",
        "            yield self.get_batch(i)\n",
        "\n",
        "    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n",
        "        max_len = self.bptt + max_deviation * std\n",
        "        i = start\n",
        "        while True:\n",
        "            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
        "            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n",
        "            data, target, seq_len = self.get_batch(i, bptt)\n",
        "            i += seq_len\n",
        "            yield data, target, seq_len\n",
        "            if i >= self.data.size(0) - 2:\n",
        "                break\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.get_fixlen_iter()\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path, dataset, *args, **kwargs):\n",
        "        self.dataset = dataset\n",
        "        self.vocab = Vocab(*args, **kwargs)\n",
        "\n",
        "        if self.dataset in ['ptb', 'wt2', 'enwik8', 'text8']:\n",
        "            self.vocab.count_file(os.path.join(path, 'train.txt'))\n",
        "            self.vocab.count_file(os.path.join(path, 'valid.txt'))\n",
        "            self.vocab.count_file(os.path.join(path, 'test.txt'))\n",
        "        elif self.dataset == 'wt103':\n",
        "            self.vocab.count_file(os.path.join(path, 'train.txt'))\n",
        "        elif self.dataset == 'lm1b':\n",
        "            train_path_pattern = os.path.join(\n",
        "                path, '1-billion-word-language-modeling-benchmark-r13output',\n",
        "                'training-monolingual.tokenized.shuffled', 'news.en-*')\n",
        "            train_paths = glob.glob(train_path_pattern)\n",
        "            # the vocab will load from file when build_vocab() is called\n",
        "\n",
        "        self.vocab.build_vocab()\n",
        "\n",
        "        if self.dataset in ['ptb', 'wt2', 'wt103']:\n",
        "            self.train = self.vocab.encode_file(\n",
        "                os.path.join(path, 'train.txt'), ordered=True)\n",
        "            self.valid = self.vocab.encode_file(\n",
        "                os.path.join(path, 'valid.txt'), ordered=True)\n",
        "            self.test  = self.vocab.encode_file(\n",
        "                os.path.join(path, 'test.txt'), ordered=True)\n",
        "        elif self.dataset in ['enwik8', 'text8']:\n",
        "            self.train = self.vocab.encode_file(\n",
        "                os.path.join(path, 'train.txt'), ordered=True, add_eos=False)\n",
        "            self.valid = self.vocab.encode_file(\n",
        "                os.path.join(path, 'valid.txt'), ordered=True, add_eos=False)\n",
        "            self.test  = self.vocab.encode_file(\n",
        "                os.path.join(path, 'test.txt'), ordered=True, add_eos=False)\n",
        "        elif self.dataset == 'lm1b':\n",
        "            self.train = train_paths\n",
        "            self.valid = self.vocab.encode_file(\n",
        "                os.path.join(path, 'valid.txt'), ordered=False, add_double_eos=True)\n",
        "            self.test  = self.vocab.encode_file(\n",
        "                os.path.join(path, 'test.txt'), ordered=False, add_double_eos=True)\n",
        "\n",
        "    def get_iterator(self, split, *args, **kwargs):\n",
        "        if split == 'train':\n",
        "            if self.dataset in ['ptb', 'wt2', 'wt103', 'enwik8', 'text8']:\n",
        "                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n",
        "            elif self.dataset == 'lm1b':\n",
        "                kwargs['shuffle'] = True\n",
        "                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n",
        "        elif split in ['valid', 'test']:\n",
        "            data = self.valid if split == 'valid' else self.test\n",
        "            if self.dataset in ['ptb', 'wt2', 'wt103', 'enwik8', 'text8']:\n",
        "                data_iter = LMOrderedIterator(data, *args, **kwargs)\n",
        "            elif self.dataset == 'lm1b':\n",
        "                data_iter = LMShuffledIterator(data, *args, **kwargs)\n",
        "\n",
        "        return data_iter\n",
        "\n",
        "\n",
        "def get_lm_corpus(datadir, dataset):\n",
        "    fn = os.path.join(datadir, 'cache.pt')\n",
        "    if os.path.exists(fn):\n",
        "        print('Loading cached dataset...')\n",
        "        corpus = torch.load(fn)\n",
        "    else:\n",
        "        print('Producing dataset {}...'.format(dataset))\n",
        "        kwargs = {}\n",
        "        if dataset in ['wt103', 'wt2']:\n",
        "            kwargs['special'] = ['<eos>']\n",
        "            kwargs['lower_case'] = False\n",
        "        elif dataset == 'ptb':\n",
        "            kwargs['special'] = ['<eos>']\n",
        "            kwargs['lower_case'] = True\n",
        "        elif dataset == 'lm1b':\n",
        "            kwargs['special'] = []\n",
        "            kwargs['lower_case'] = False\n",
        "            kwargs['vocab_file'] = os.path.join(datadir, '1b_word_vocab.txt')\n",
        "        elif dataset in ['enwik8', 'text8']:\n",
        "            pass\n",
        "\n",
        "        corpus = Corpus(datadir, dataset, **kwargs)\n",
        "        torch.save(corpus, fn)\n",
        "\n",
        "    return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIh5g8yj2G3t",
        "colab_type": "text"
      },
      "source": [
        "### Function to calculate number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmGH7Zd-2F6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def n_params(model):\n",
        "    trainable = filter(lambda x: x.requires_grad, model.parameters())\n",
        "    n_params = sum([np.prod(p.size()) for p in trainable])\n",
        "    return n_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeFa1JAZ0eE5",
        "colab_type": "text"
      },
      "source": [
        "# Transformer Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NZ_6XM9xQxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Self-attention decoder layer.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout_ff, dropout_att):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.dropout_ff = nn.Dropout(dropout_ff)\n",
        "        self.dropout_att = nn.Dropout(dropout_att)\n",
        "        assert d_model % n_heads == 0\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.ffn1 = nn.Linear(d_model, d_ff)\n",
        "        self.ffn2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, return_affinity=False):\n",
        "        \"\"\"Multi-head attention.\"\"\"\n",
        "        # x (B, emb, seqlen)\n",
        "        bsz, _, seqlen = x.size()\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        \"\"\"Compute attention matrix.\"\"\"\n",
        "        q = self.wq(x).view(bsz, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.wk(x).view(bsz, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.wv(x).view(bsz, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        attn_mat = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.d_head)\n",
        "        # attn_mat (B, num_heads, seqlen, seqlen)\n",
        "\n",
        "        \"\"\"Mask future tokens.\"\"\"\n",
        "        mask = torch.triu(torch.ones([seqlen, seqlen]), diagonal=1)[None, None, :, :]\n",
        "        mask[mask.bool()] = -np.inf\n",
        "        attn_mat = attn_mat + mask.to(device)\n",
        "        \n",
        "        \"\"\"Calculate affinities.\"\"\"\n",
        "        affinity = F.softmax(attn_mat, dim=-1)\n",
        "        \n",
        "        \"\"\"Compute new representations.\"\"\"\n",
        "        weighted = torch.matmul(self.dropout_att(affinity), v)\n",
        "        # weighted (B, num_heads, seqlen, d_head)\n",
        "        merged = (weighted.transpose(1, 2).contiguous()\n",
        "            .view(bsz, -1, self.d_model))\n",
        "        # merged (B, seqlen, d_model)\n",
        "        out = self.fc(merged)\n",
        "        out = self.dropout_ff(out)\n",
        "        attn_out = self.norm1(out + x)\n",
        "\n",
        "        \"\"\"Positionwise feed-forward network.\"\"\"\n",
        "        out = self.ffn2(F.relu(self.ffn1(attn_out)))\n",
        "        # out (B, seqlen, d_model)\n",
        "        out = self.dropout_ff(out)\n",
        "        ffn_out = self.norm2(out + attn_out)\n",
        "        ffn_out = ffn_out.transpose(1, 2)\n",
        "        # ffn_out (B, d_model, seqlen)\n",
        "\n",
        "        if return_affinity:\n",
        "            return ffn_out, affinity\n",
        "        return ffn_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EzwVJuhxQxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.encoding = PositionalEncoding(d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # (seqlen, B)\n",
        "        x = self.embedding(x)\n",
        "        # (seqlen, B, d_model)\n",
        "        x = x.permute(1, 2, 0)\n",
        "        # (B, d_model, seqlen)\n",
        "        x = self.encoding(x)\n",
        "        x = self.norm(x.transpose(1, 2))\n",
        "        return x.transpose(1, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81vyoDSRxQxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OutputProjection(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super(OutputProjection, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.proj(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGnQY5rWxQxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.inv_freq_sin = 1. / (10000 ** (torch.arange(0., d_model, 2.) / d_model))\n",
        "        self.inv_freq_cos = 1. / (10000 ** (torch.arange(1., d_model, 2.) / d_model))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        bsz, emb, seqlen = x.size()\n",
        "        assert emb == self.d_model\n",
        "        pos_seq = torch.arange(0., seqlen, 1.)\n",
        "        sin_inp = torch.ger(pos_seq, self.inv_freq_sin).sin()\n",
        "        cos_inp = torch.ger(pos_seq, self.inv_freq_cos).cos()\n",
        "        pos_enc = torch.stack((sin_inp, cos_inp), dim=2).view(1, seqlen, emb)\n",
        "        merged = np.sqrt(self.d_model) * x + pos_enc.transpose(1, 2).to(device)\n",
        "        return merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYfiZ-txQxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, d_inner, n_heads, n_layers, \n",
        "                 dropout_ff, dropout_att):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.d_inner = d_inner\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ff = dropout_ff\n",
        "        self.dropout_att = dropout_att\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(Embedding(vocab_size, d_model))\n",
        "        self.layers.extend([Attention(d_model, n_heads, d_inner, dropout_ff, \n",
        "                                      dropout_att) for _ in range(n_layers)])\n",
        "        self.layers.append(OutputProjection(d_model, vocab_size))\n",
        "    \n",
        "    def affinity(self, x):\n",
        "        affs = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Attention):\n",
        "                x, aff = layer(x, return_affinity=True)\n",
        "                affs.append(aff[:, None, :, :, :])\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        # (B, nlayers, nheads, seqlen, seqlen)\n",
        "        affs = torch.cat(affs, 1)\n",
        "        return affs\n",
        "            \n",
        "    def _forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        pred = self._forward(x).contiguous()\n",
        "        pred = pred.view(-1, self.vocab_size)\n",
        "        # pred (B * seqlen, vocab_size)\n",
        "        y = y.t().contiguous().view(-1)\n",
        "        # y (B * seqlen)\n",
        "        loss = F.cross_entropy(pred, y)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5wQXy5E0SIa",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJdLbX_2xQxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_MODEL = 256\n",
        "D_INNER = 512\n",
        "N_HEADS = 4\n",
        "N_LAYERS = 4\n",
        "DROPOUT_FF = 0.1\n",
        "DROPOUT_ATT = 0.0\n",
        "BATCH_SIZE = 20\n",
        "MAX_SEQLEN = 128\n",
        "LEARNING_RATE = 2.5e-4\n",
        "ANNEAL_STEPS = 400000\n",
        "GRAD_CLIP = 0.25\n",
        "LOG_INTERVAL = 100\n",
        "EVAL_INTERVAL = 2000\n",
        "N_EPOCHS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "539RD5y90WTS",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wKngagXxQxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Js23ZCKFjYz",
        "colab_type": "text"
      },
      "source": [
        "Here we produce our language modeling corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUfper3V6KlF",
        "colab_type": "code",
        "outputId": "edd9a163-d993-445a-86aa-969376d6308e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus = get_lm_corpus('./', 'enwik8')\n",
        "VOCAB_SIZE = len(corpus.vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC7sjaFvFowP",
        "colab_type": "text"
      },
      "source": [
        "And create three iterators: one for each dataset (training, validation, testing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5M8kMh46NNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = corpus.get_iterator('train', BATCH_SIZE, MAX_SEQLEN, device)\n",
        "valid_iter = corpus.get_iterator('valid', BATCH_SIZE, MAX_SEQLEN, device)\n",
        "test_iter = corpus.get_iterator('test', BATCH_SIZE, MAX_SEQLEN, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZysJucaTFuCY",
        "colab_type": "text"
      },
      "source": [
        "Now let's make our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxafJYUJ6TMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Transformer(vocab_size=VOCAB_SIZE, d_model=D_MODEL, d_inner=D_INNER, \n",
        "                    n_heads=N_HEADS, n_layers=N_LAYERS, dropout_ff=DROPOUT_FF,\n",
        "                    dropout_att=DROPOUT_ATT).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qQ0EOsiFwBf",
        "colab_type": "text"
      },
      "source": [
        "And initialize the weights of the network so that the variance at the input of each hidden layer is near unity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVTTVrqpEHol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DQOc6AXBOWu",
        "colab_type": "code",
        "outputId": "6d928f84-fee1-4a44-f374-aeebe3c6abce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Model created. {} trainable parameters.\".format(n_params(model)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model created. 2213580 trainable parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkRkZOA7GOpC",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Eqggov5TPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src, tgt, seq_len = next(iter(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMC1LKf0GUVP",
        "colab_type": "code",
        "outputId": "b81c91cb-1e70-4a11-9f46-fc7ec1cbfe83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(src.shape, tgt.shape, seq_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 20]) torch.Size([128, 20]) 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EK3O2JTGRWb",
        "colab_type": "text"
      },
      "source": [
        "Notice the source and target are (seqlen, B) in shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3qZRnez5XNj",
        "colab_type": "code",
        "outputId": "ccedfb47-5f94-4b7b-a9f9-10d65371fb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(src[:10, 0], tgt[:10, 0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([41, 16,  1, 11,  4,  3, 23,  4, 32,  4], device='cuda:0') tensor([16,  1, 11,  4,  3, 23,  4, 32,  4,  0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGQeNIsRGc-l",
        "colab_type": "text"
      },
      "source": [
        "We can also see explicitly that the target for a given source is simply the source shifted over by one character i.e. given an input character our aim is to predict the subsequent character at each time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmFxuzcHGrrO",
        "colab_type": "text"
      },
      "source": [
        "It's also good practice to check that we get a reasonable loss after randomly initializing the network. Since we output a discrete distribution over `VOCAB_SIZE` values, we expect initially to be guessing at random with a probability `1/VOCAB_SIZE` of guessing correctly. This corresponds to a cross entropy loss of `-ln(1/VOCAB_SIZE)`. Let's make sure we find something close."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9hfEu4EKQzI",
        "colab_type": "code",
        "outputId": "2a60e097-c8c4-49e5-b15d-e07546e63e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Model loss: {:.3f} | Random guessing loss: {:.3f}'.format(model(src, tgt).item(), -np.log(1./VOCAB_SIZE)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loss: 5.802 | Random guessing loss: 5.318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ifBv3tOxQxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, ANNEAL_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-9DGv9xxQx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "best_val_loss = np.inf\n",
        "metrics = col.defaultdict(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfleaMPpxQx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    global best_val_loss, metrics, global_step\n",
        "    \n",
        "    for epoch in range(N_EPOCHS):\n",
        "        for batch, (src, tgt, seqlen) in enumerate(train_iter):\n",
        "            model.zero_grad()\n",
        "            loss = model(src, tgt)\n",
        "            loss = loss.float().mean()\n",
        "            train_loss += loss.float().item()\n",
        "            loss.backward()\n",
        "            U.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "\n",
        "            if batch % LOG_INTERVAL == 0 and batch != 0:\n",
        "                cur_loss = train_loss / LOG_INTERVAL\n",
        "                metrics['train_gs'].append(global_step)\n",
        "                metrics['train_loss'].append(cur_loss)\n",
        "                print('Epoch: {} | Batch: {} | Current Loss: {}'.format(\n",
        "                    epoch, batch, cur_loss))\n",
        "                train_loss = 0\n",
        "\n",
        "            if batch % EVAL_INTERVAL == 0 and batch != 0:\n",
        "                evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcHEbw3v7ln5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    global best_val_loss, metrics, global_step\n",
        "    \n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (src, tgt, seqlen) in enumerate(valid_iter):\n",
        "            loss = model(src, tgt)\n",
        "            loss = loss.float().mean()\n",
        "            val_loss += loss.float().item()\n",
        "    val_loss = val_loss / (batch + 1)\n",
        "    \n",
        "    print('=' * 100)\n",
        "    print('Current validation loss: {}'.format(val_loss))\n",
        "    print('=' * 100)\n",
        "\n",
        "    metrics['valid_gs'].append(global_step)\n",
        "    metrics['valid_loss'].append(val_loss)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        with open('model.pt', 'wb') as f:\n",
        "            torch.save(model.state_dict(), f)\n",
        "    \n",
        "    with open('metrics.pkl', 'wb') as f:\n",
        "      pickle.dump(metrics, f)\n",
        "            \n",
        "    model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC3_LjtyPWE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(seed, max_len=128, sample=False, temperature=1.):\n",
        "    model.eval()\n",
        "\n",
        "    n = len(seed)\n",
        "    seed = [corpus.vocab.get_idx(str(ord(c))) for c in seed]\n",
        "    src = torch.LongTensor(seed).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Autoregressive sampling\n",
        "      for i in range(max_len - n):\n",
        "          pred = model._forward(src)\n",
        "          if sample:\n",
        "            dist = F.softmax(pred / temperature, -1)\n",
        "            c = torch.distributions.Categorical(dist)\n",
        "            words = c.sample().t()\n",
        "          else:\n",
        "            words = pred.argmax(-1).t()\n",
        "          src = torch.cat([src, words[-1:]], 0)\n",
        "    \n",
        "    src = src.detach().cpu().numpy().flatten()\n",
        "    out = [corpus.vocab.get_sym(idx) for idx in src]\n",
        "    phrase = ''.join([chr(int(c)) for c in out])\n",
        "    \n",
        "    return phrase"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ3yz7SzIHPT",
        "colab_type": "text"
      },
      "source": [
        "Now let's start training. At each evaluation step, the model will be saved if it performs better than at the last evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GcTFP9Z8xQyC",
        "colab_type": "code",
        "outputId": "0357a261-a87a-468f-f461-86dcbaa69975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "try:\n",
        "    train()\n",
        "except KeyboardInterrupt:\n",
        "    save = input('Aborting training. Save model (y/n)? ')\n",
        "    if save.lower() == 'y':\n",
        "        with open('model.pt', 'wb') as f:\n",
        "            torch.save(model.state_dict(), f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Batch: 100 | Current Loss: 3.1273014426231383\n",
            "Epoch: 0 | Batch: 200 | Current Loss: 2.6969605135917663\n",
            "Epoch: 0 | Batch: 300 | Current Loss: 2.5829190254211425\n",
            "Epoch: 0 | Batch: 400 | Current Loss: 2.421724419593811\n",
            "Epoch: 0 | Batch: 500 | Current Loss: 2.427252297401428\n",
            "Epoch: 0 | Batch: 600 | Current Loss: 2.3510650539398195\n",
            "Epoch: 0 | Batch: 700 | Current Loss: 2.3072413754463197\n",
            "Epoch: 0 | Batch: 800 | Current Loss: 2.2590156507492067\n",
            "Epoch: 0 | Batch: 900 | Current Loss: 2.21658931016922\n",
            "Epoch: 0 | Batch: 1000 | Current Loss: 2.137557135820389\n",
            "Epoch: 0 | Batch: 1100 | Current Loss: 2.1404385566711426\n",
            "Epoch: 0 | Batch: 1200 | Current Loss: 2.002024199962616\n",
            "Epoch: 0 | Batch: 1300 | Current Loss: 1.9997404778003693\n",
            "Epoch: 0 | Batch: 1400 | Current Loss: 2.0569986653327943\n",
            "Epoch: 0 | Batch: 1500 | Current Loss: 1.9676348781585693\n",
            "Epoch: 0 | Batch: 1600 | Current Loss: 2.0019256353378294\n",
            "Epoch: 0 | Batch: 1700 | Current Loss: 1.9734273076057434\n",
            "Epoch: 0 | Batch: 1800 | Current Loss: 1.9749938583374023\n",
            "Epoch: 0 | Batch: 1900 | Current Loss: 1.9780888426303864\n",
            "Epoch: 0 | Batch: 2000 | Current Loss: 1.9354206454753875\n",
            "====================================================================================================\n",
            "Current validation loss: 1.935152755699296\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 2100 | Current Loss: 1.941055153608322\n",
            "Epoch: 0 | Batch: 2200 | Current Loss: 1.9004135346412658\n",
            "Epoch: 0 | Batch: 2300 | Current Loss: 1.9130778658390044\n",
            "Epoch: 0 | Batch: 2400 | Current Loss: 1.8450869035720825\n",
            "Epoch: 0 | Batch: 2500 | Current Loss: 1.847847752571106\n",
            "Epoch: 0 | Batch: 2600 | Current Loss: 1.8573704588413238\n",
            "Epoch: 0 | Batch: 2700 | Current Loss: 1.8745335245132446\n",
            "Epoch: 0 | Batch: 2800 | Current Loss: 1.7867811501026154\n",
            "Epoch: 0 | Batch: 2900 | Current Loss: 1.7976339173316955\n",
            "Epoch: 0 | Batch: 3000 | Current Loss: 1.769605736732483\n",
            "Epoch: 0 | Batch: 3100 | Current Loss: 1.7842361092567445\n",
            "Epoch: 0 | Batch: 3200 | Current Loss: 1.7755753409862518\n",
            "Epoch: 0 | Batch: 3300 | Current Loss: 1.759651973247528\n",
            "Epoch: 0 | Batch: 3400 | Current Loss: 1.7486023652553557\n",
            "Epoch: 0 | Batch: 3500 | Current Loss: 1.7444587290287017\n",
            "Epoch: 0 | Batch: 3600 | Current Loss: 1.7483750700950622\n",
            "Epoch: 0 | Batch: 3700 | Current Loss: 1.7436928272247314\n",
            "Epoch: 0 | Batch: 3800 | Current Loss: 1.7178388774394988\n",
            "Epoch: 0 | Batch: 3900 | Current Loss: 1.7377776205539703\n",
            "Epoch: 0 | Batch: 4000 | Current Loss: 1.7106336593627929\n",
            "====================================================================================================\n",
            "Current validation loss: 1.7556473234548826\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 4100 | Current Loss: 1.7093367266654969\n",
            "Epoch: 0 | Batch: 4200 | Current Loss: 1.7541184186935426\n",
            "Epoch: 0 | Batch: 4300 | Current Loss: 1.7457157516479491\n",
            "Epoch: 0 | Batch: 4400 | Current Loss: 1.696532644033432\n",
            "Epoch: 0 | Batch: 4500 | Current Loss: 1.7045297563076018\n",
            "Epoch: 0 | Batch: 4600 | Current Loss: 1.6768143308162688\n",
            "Epoch: 0 | Batch: 4700 | Current Loss: 1.6758216977119447\n",
            "Epoch: 0 | Batch: 4800 | Current Loss: 1.6867934048175812\n",
            "Epoch: 0 | Batch: 4900 | Current Loss: 1.6548079347610474\n",
            "Epoch: 0 | Batch: 5000 | Current Loss: 1.6640698194503785\n",
            "Epoch: 0 | Batch: 5100 | Current Loss: 1.6717768454551696\n",
            "Epoch: 0 | Batch: 5200 | Current Loss: 1.6409277677536012\n",
            "Epoch: 0 | Batch: 5300 | Current Loss: 1.6404284596443177\n",
            "Epoch: 0 | Batch: 5400 | Current Loss: 1.6769454181194305\n",
            "Epoch: 0 | Batch: 5500 | Current Loss: 1.6673563969135285\n",
            "Epoch: 0 | Batch: 5600 | Current Loss: 1.5853055679798127\n",
            "Epoch: 0 | Batch: 5700 | Current Loss: 1.6360572373867035\n",
            "Epoch: 0 | Batch: 5800 | Current Loss: 1.609270828962326\n",
            "Epoch: 0 | Batch: 5900 | Current Loss: 1.6198496687412263\n",
            "Epoch: 0 | Batch: 6000 | Current Loss: 1.6207854211330415\n",
            "====================================================================================================\n",
            "Current validation loss: 1.6603571918193352\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 6100 | Current Loss: 1.6262559044361113\n",
            "Epoch: 0 | Batch: 6200 | Current Loss: 1.594134625196457\n",
            "Epoch: 0 | Batch: 6300 | Current Loss: 1.6329087269306184\n",
            "Epoch: 0 | Batch: 6400 | Current Loss: 1.6427925765514373\n",
            "Epoch: 0 | Batch: 6500 | Current Loss: 1.5944150388240814\n",
            "Epoch: 0 | Batch: 6600 | Current Loss: 1.5705075478553772\n",
            "Epoch: 0 | Batch: 6700 | Current Loss: 1.5840156817436217\n",
            "Epoch: 0 | Batch: 6800 | Current Loss: 1.5800951623916626\n",
            "Epoch: 0 | Batch: 6900 | Current Loss: 1.5873193514347077\n",
            "Epoch: 0 | Batch: 7000 | Current Loss: 1.6189017069339753\n",
            "Epoch: 0 | Batch: 7100 | Current Loss: 1.560485179424286\n",
            "Epoch: 0 | Batch: 7200 | Current Loss: 1.5748322236537933\n",
            "Epoch: 0 | Batch: 7300 | Current Loss: 1.50560063123703\n",
            "Epoch: 0 | Batch: 7400 | Current Loss: 1.5312039494514464\n",
            "Epoch: 0 | Batch: 7500 | Current Loss: 1.5977616012096405\n",
            "Epoch: 0 | Batch: 7600 | Current Loss: 1.6006001818180084\n",
            "Epoch: 0 | Batch: 7700 | Current Loss: 1.5812259387969971\n",
            "Epoch: 0 | Batch: 7800 | Current Loss: 1.6344062221050262\n",
            "Epoch: 0 | Batch: 7900 | Current Loss: 1.5873290276527405\n",
            "Epoch: 0 | Batch: 8000 | Current Loss: 1.5814282619953155\n",
            "====================================================================================================\n",
            "Current validation loss: 1.6075148277030968\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 8100 | Current Loss: 1.550431377887726\n",
            "Epoch: 0 | Batch: 8200 | Current Loss: 1.5392478144168853\n",
            "Epoch: 0 | Batch: 8300 | Current Loss: 1.5117796206474303\n",
            "Epoch: 0 | Batch: 8400 | Current Loss: 1.5292857253551484\n",
            "Epoch: 0 | Batch: 8500 | Current Loss: 1.5642453479766845\n",
            "Epoch: 0 | Batch: 8600 | Current Loss: 1.5962520921230317\n",
            "Epoch: 0 | Batch: 8700 | Current Loss: 1.5510613203048706\n",
            "Epoch: 0 | Batch: 8800 | Current Loss: 1.5607305109500884\n",
            "Epoch: 0 | Batch: 8900 | Current Loss: 1.55872301697731\n",
            "Epoch: 0 | Batch: 9000 | Current Loss: 1.5496169662475585\n",
            "Epoch: 0 | Batch: 9100 | Current Loss: 1.5924357128143312\n",
            "Epoch: 0 | Batch: 9200 | Current Loss: 1.5961043775081634\n",
            "Epoch: 0 | Batch: 9300 | Current Loss: 1.5881911659240722\n",
            "Epoch: 0 | Batch: 9400 | Current Loss: 1.5643104362487792\n",
            "Epoch: 0 | Batch: 9500 | Current Loss: 1.53173308968544\n",
            "Epoch: 0 | Batch: 9600 | Current Loss: 1.543973684310913\n",
            "Epoch: 0 | Batch: 9700 | Current Loss: 1.5293393850326538\n",
            "Epoch: 0 | Batch: 9800 | Current Loss: 1.5492566049098968\n",
            "Epoch: 0 | Batch: 9900 | Current Loss: 1.5682458674907684\n",
            "Epoch: 0 | Batch: 10000 | Current Loss: 1.5254571068286895\n",
            "====================================================================================================\n",
            "Current validation loss: 1.552823358502694\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 10100 | Current Loss: 1.49083340883255\n",
            "Epoch: 0 | Batch: 10200 | Current Loss: 1.5202400028705596\n",
            "Epoch: 0 | Batch: 10300 | Current Loss: 1.5297537684440612\n",
            "Epoch: 0 | Batch: 10400 | Current Loss: 1.4795089077949524\n",
            "Epoch: 0 | Batch: 10500 | Current Loss: 1.475510791540146\n",
            "Epoch: 0 | Batch: 10600 | Current Loss: 1.449950269460678\n",
            "Epoch: 0 | Batch: 10700 | Current Loss: 1.444039338827133\n",
            "Epoch: 0 | Batch: 10800 | Current Loss: 1.4792377734184265\n",
            "Epoch: 0 | Batch: 10900 | Current Loss: 1.4690390574932097\n",
            "Epoch: 0 | Batch: 11000 | Current Loss: 1.4786875212192536\n",
            "Epoch: 0 | Batch: 11100 | Current Loss: 1.5023751521110535\n",
            "Epoch: 0 | Batch: 11200 | Current Loss: 1.5064170491695403\n",
            "Epoch: 0 | Batch: 11300 | Current Loss: 1.4717069637775422\n",
            "Epoch: 0 | Batch: 11400 | Current Loss: 1.4852322518825531\n",
            "Epoch: 0 | Batch: 11500 | Current Loss: 1.5134225356578828\n",
            "Epoch: 0 | Batch: 11600 | Current Loss: 1.4537660253047944\n",
            "Epoch: 0 | Batch: 11700 | Current Loss: 1.5091834938526154\n",
            "Epoch: 0 | Batch: 11800 | Current Loss: 1.5203972089290618\n",
            "Epoch: 0 | Batch: 11900 | Current Loss: 1.4807804143428802\n",
            "Epoch: 0 | Batch: 12000 | Current Loss: 1.4921853601932527\n",
            "====================================================================================================\n",
            "Current validation loss: 1.5339426018189692\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 12100 | Current Loss: 1.4617047452926635\n",
            "Epoch: 0 | Batch: 12200 | Current Loss: 1.4856040966510773\n",
            "Epoch: 0 | Batch: 12300 | Current Loss: 1.4722263848781585\n",
            "Epoch: 0 | Batch: 12400 | Current Loss: 1.4610916519165038\n",
            "Epoch: 0 | Batch: 12500 | Current Loss: 1.4651795732975006\n",
            "Epoch: 0 | Batch: 12600 | Current Loss: 1.4893112754821778\n",
            "Epoch: 0 | Batch: 12700 | Current Loss: 1.4990079152584075\n",
            "Epoch: 0 | Batch: 12800 | Current Loss: 1.4820634388923646\n",
            "Epoch: 0 | Batch: 12900 | Current Loss: 1.474899023771286\n",
            "Epoch: 0 | Batch: 13000 | Current Loss: 1.4531887578964233\n",
            "Epoch: 0 | Batch: 13100 | Current Loss: 1.4470034980773925\n",
            "Epoch: 0 | Batch: 13200 | Current Loss: 1.4985140478610992\n",
            "Epoch: 0 | Batch: 13300 | Current Loss: 1.4612150084972382\n",
            "Epoch: 0 | Batch: 13400 | Current Loss: 1.504299750328064\n",
            "Epoch: 0 | Batch: 13500 | Current Loss: 1.4988817620277404\n",
            "Epoch: 0 | Batch: 13600 | Current Loss: 1.5074329996109008\n",
            "Epoch: 0 | Batch: 13700 | Current Loss: 1.4728126776218415\n",
            "Epoch: 0 | Batch: 13800 | Current Loss: 1.4916130495071411\n",
            "Epoch: 0 | Batch: 13900 | Current Loss: 1.502769936323166\n",
            "Epoch: 0 | Batch: 14000 | Current Loss: 1.4354972207546235\n",
            "====================================================================================================\n",
            "Current validation loss: 1.5157664088729006\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 14100 | Current Loss: 1.4802524507045747\n",
            "Epoch: 0 | Batch: 14200 | Current Loss: 1.5153167676925658\n",
            "Epoch: 0 | Batch: 14300 | Current Loss: 1.4587979435920715\n",
            "Epoch: 0 | Batch: 14400 | Current Loss: 1.4493922579288483\n",
            "Epoch: 0 | Batch: 14500 | Current Loss: 1.4856478261947632\n",
            "Epoch: 0 | Batch: 14600 | Current Loss: 1.47320792555809\n",
            "Epoch: 0 | Batch: 14700 | Current Loss: 1.47033163189888\n",
            "Epoch: 0 | Batch: 14800 | Current Loss: 1.4489573180675506\n",
            "Epoch: 0 | Batch: 14900 | Current Loss: 1.4603834056854248\n",
            "Epoch: 0 | Batch: 15000 | Current Loss: 1.4619188976287842\n",
            "Epoch: 0 | Batch: 15100 | Current Loss: 1.4547647213935853\n",
            "Epoch: 0 | Batch: 15200 | Current Loss: 1.4525069522857665\n",
            "Epoch: 0 | Batch: 15300 | Current Loss: 1.4843017387390136\n",
            "Epoch: 0 | Batch: 15400 | Current Loss: 1.4262733817100526\n",
            "Epoch: 0 | Batch: 15500 | Current Loss: 1.452733677625656\n",
            "Epoch: 0 | Batch: 15600 | Current Loss: 1.4892822062969209\n",
            "Epoch: 0 | Batch: 15700 | Current Loss: 1.4652274763584137\n",
            "Epoch: 0 | Batch: 15800 | Current Loss: 1.4538303351402282\n",
            "Epoch: 0 | Batch: 15900 | Current Loss: 1.4503388833999633\n",
            "Epoch: 0 | Batch: 16000 | Current Loss: 1.494672235250473\n",
            "====================================================================================================\n",
            "Current validation loss: 1.4896534571489686\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 16100 | Current Loss: 1.4747782278060912\n",
            "Epoch: 0 | Batch: 16200 | Current Loss: 1.4348136842250825\n",
            "Epoch: 0 | Batch: 16300 | Current Loss: 1.4347857642173767\n",
            "Epoch: 0 | Batch: 16400 | Current Loss: 1.4884403848648071\n",
            "Epoch: 0 | Batch: 16500 | Current Loss: 1.4655007421970367\n",
            "Epoch: 0 | Batch: 16600 | Current Loss: 1.4417654907703399\n",
            "Epoch: 0 | Batch: 16700 | Current Loss: 1.4218163681030274\n",
            "Epoch: 0 | Batch: 16800 | Current Loss: 1.4693097734451295\n",
            "Epoch: 0 | Batch: 16900 | Current Loss: 1.4613477313518524\n",
            "Epoch: 0 | Batch: 17000 | Current Loss: 1.4605546545982362\n",
            "Epoch: 0 | Batch: 17100 | Current Loss: 1.4547018682956696\n",
            "Epoch: 0 | Batch: 17200 | Current Loss: 1.4500034713745118\n",
            "Epoch: 0 | Batch: 17300 | Current Loss: 1.4384850823879243\n",
            "Epoch: 0 | Batch: 17400 | Current Loss: 1.4506917822360992\n",
            "Epoch: 0 | Batch: 17500 | Current Loss: 1.4686486732959747\n",
            "Epoch: 0 | Batch: 17600 | Current Loss: 1.436964020729065\n",
            "Epoch: 0 | Batch: 17700 | Current Loss: 1.445584363937378\n",
            "Epoch: 0 | Batch: 17800 | Current Loss: 1.4357962501049042\n",
            "Epoch: 0 | Batch: 17900 | Current Loss: 1.4697344267368317\n",
            "Epoch: 0 | Batch: 18000 | Current Loss: 1.4191354930400848\n",
            "====================================================================================================\n",
            "Current validation loss: 1.4829577587035871\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 18100 | Current Loss: 1.4235898411273957\n",
            "Epoch: 0 | Batch: 18200 | Current Loss: 1.4264773726463318\n",
            "Epoch: 0 | Batch: 18300 | Current Loss: 1.4290114891529084\n",
            "Epoch: 0 | Batch: 18400 | Current Loss: 1.4506714749336242\n",
            "Epoch: 0 | Batch: 18500 | Current Loss: 1.465224461555481\n",
            "Epoch: 0 | Batch: 18600 | Current Loss: 1.4779650032520295\n",
            "Epoch: 0 | Batch: 18700 | Current Loss: 1.4529461884498596\n",
            "Epoch: 0 | Batch: 18800 | Current Loss: 1.45535391330719\n",
            "Epoch: 0 | Batch: 18900 | Current Loss: 1.426776807308197\n",
            "Epoch: 0 | Batch: 19000 | Current Loss: 1.402857654094696\n",
            "Epoch: 0 | Batch: 19100 | Current Loss: 1.4448728835582734\n",
            "Epoch: 0 | Batch: 19200 | Current Loss: 1.4885092687606811\n",
            "Epoch: 0 | Batch: 19300 | Current Loss: 1.5080985271930694\n",
            "Epoch: 0 | Batch: 19400 | Current Loss: 1.4356780445575714\n",
            "Epoch: 0 | Batch: 19500 | Current Loss: 1.4041831696033478\n",
            "Epoch: 0 | Batch: 19600 | Current Loss: 1.4419823110103607\n",
            "Epoch: 0 | Batch: 19700 | Current Loss: 1.443686887025833\n",
            "Epoch: 0 | Batch: 19800 | Current Loss: 1.4038594031333924\n",
            "Epoch: 0 | Batch: 19900 | Current Loss: 1.4265086376667022\n",
            "Epoch: 0 | Batch: 20000 | Current Loss: 1.3240291035175324\n",
            "====================================================================================================\n",
            "Current validation loss: 1.4665696203215028\n",
            "====================================================================================================\n",
            "Epoch: 0 | Batch: 20100 | Current Loss: 1.391676776409149\n",
            "Epoch: 0 | Batch: 20200 | Current Loss: 1.4161582100391388\n",
            "Epoch: 0 | Batch: 20300 | Current Loss: 1.4279485750198364\n",
            "Epoch: 0 | Batch: 20400 | Current Loss: 1.401574853658676\n",
            "Epoch: 0 | Batch: 20500 | Current Loss: 1.4351141774654388\n",
            "Epoch: 0 | Batch: 20600 | Current Loss: 1.4252294933795928\n",
            "Epoch: 0 | Batch: 20700 | Current Loss: 1.4106468451023102\n",
            "Epoch: 0 | Batch: 20800 | Current Loss: 1.3801986432075501\n",
            "Aborting training. Save model (y/n)? n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbJryJaPaqu",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize our training curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFV8UmIwPZNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('metrics.pkl', 'rb') as f:\n",
        "  m = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXsC9pVNPjy0",
        "colab_type": "code",
        "outputId": "5e8a05ad-81fb-4ee7-88b2-518a4d28d5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "plt.plot(m['train_gs'], m['train_loss'], label='Training Loss')\n",
        "plt.plot(m['valid_gs'], m['valid_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA60AAAHUCAYAAADRF87/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0LU2MTmIFDI",
        "colab_type": "text"
      },
      "source": [
        "Now let's generate some sentences. First, we'll load the best saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5vuqQW2xQyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('model.pt', 'rb') as f:\n",
        "  model.load_state_dict(torch.load(f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cJLOjblP2aG",
        "colab_type": "text"
      },
      "source": [
        "Then we'll choose a seed and repeatedly sample from the Transformer. Remember, it will use its own outputs to generate new outputs since it is an autoregressive model. This means we pass in a seed, have it make a prediction, append that to the end of the seed, and repeat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDsApXc9JZFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 'The Milky Way '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgcy1SuVJOoA",
        "colab_type": "code",
        "outputId": "04215a6d-9b9f-4290-d257-ca208920fda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate(seed, max_len=100, sample=True, temperature=0.8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Milky Way Twin each field in Space of State do not parallel in the Imperial Night's wide, in 199\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9svp9CPwX2XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}